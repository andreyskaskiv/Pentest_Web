#!/usr/bin/python

import requests
from alive_progress import alive_bar

from configurations.reading_argparse import get_argparse
from handlers.file_handler import (read_data_from_file,
                                   write_data_to_file)


def request(url_for_test: str):
    """Checks for the existence of a subdomain"""
    try:
        return requests.get("http://" + url_for_test)
    except requests.exceptions.ConnectionError:
        pass


def crawler(target_url: str, subdomains_list: list[str]):
    """Checks the target_url with subdomains from the list. """
    subdomains = []
    total_worlds = len(subdomains_list)

    with alive_bar(total_worlds, title='Processing') as bar:
        for line in subdomains_list:
            test_url = f"{line}.{target_url}"
            response = request(test_url)
            if response:
                print(f"[+] Discovered subdomain ----> {test_url}")
                subdomains.append(test_url)

                bar()

    return subdomains


def main(target_url: str, wordlist: str, write_file: str = 'subdomains_existing'):
    """Main controller"""
    list_of_subdomains = read_data_from_file(wordlist)
    existing_subdomains = crawler(target_url, list_of_subdomains)
    print(write_data_to_file(existing_subdomains, write_file))


if __name__ == '__main__':
    url, wordlist_file, filename_to_write = get_argparse()
    main(url, wordlist_file, filename_to_write)
