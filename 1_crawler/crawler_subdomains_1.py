#!/usr/bin/python

import requests


def request(url_for_test: str):
    """Checks for the existence of a subdomain"""
    try:
        return requests.get("http://" + url_for_test)
    except requests.exceptions.ConnectionError:
        pass


def write_data_to_file(subdomains: list, write_file: str):
    """Write subdomains into file"""
    with open(write_file, 'w') as file_to_write:
        for count_line, line in enumerate(subdomains, 1):
            row = f'{count_line}. {line}\n'
            file_to_write.write(row)


def crawler(target_url: str, subdomain_list: str):
    """Checks the target_url with subdomains from the list. """
    subdomains = []
    with open(subdomain_list, "r") as wordlist_file:
        for line in wordlist_file:
            word = line.strip()
            test_url = f"{word}.{target_url}"
            response = request(test_url)
            if response:
                print(f"[+] Discovered subdomain ----> {test_url}")
                subdomains.append(test_url)

    return subdomains


def main(target_url: str, dictionary: str, write_file: str = 'subdomains_existing.txt'):
    """Main controller"""
    existing_subdomains = crawler(target_url, dictionary)
    # print(existing_subdomains)
    write_data_to_file(existing_subdomains, write_file)


if __name__ == '__main__':
    url = "google.com"
    list_of_subdomains = "subdomain.list"

    main(url, list_of_subdomains)
