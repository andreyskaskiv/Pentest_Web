#!/usr/bin/python

import requests
from alive_progress import alive_bar

from configurations.reading_argparse import get_argparse
from handlers.file_handler import (read_data_from_file,
                                   write_data_to_file)


def request(url_for_test: str):
    """Checks for the existence of a subdomain"""
    try:
        return requests.get("http://" + url_for_test)
    except requests.exceptions.ConnectionError:
        pass


def crawler(target_url: str, directories_list: list[str]):
    """Checks the target_url with directories from the list. """
    directories = []
    total_worlds = len(directories_list)

    with alive_bar(total_worlds, title='Processing') as bar:
        for line in directories_list:
            test_url = f"{target_url}/{line}"

            response = request(test_url)

            if response:
                print(f"[+] Discovered URL ----> {test_url}")
                directories.append(test_url)
                bar()

    return directories


def main(target_url: str, wordlist: str, write_file: str = 'directories_existing'):
    """Main controller"""
    list_of_directories = read_data_from_file(wordlist)
    existing_directories = crawler(target_url, list_of_directories)
    print(write_data_to_file(existing_directories, write_file))


if __name__ == '__main__':
    url, wordlist_file, filename_to_write = get_argparse()
    main(url, wordlist_file, filename_to_write)
