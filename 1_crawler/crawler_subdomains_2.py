#!/usr/bin/python

import requests
from handlers.file_handler import read_data_from_file
from alive_progress import alive_bar

def request(url_for_test: str):
    """Checks for the existence of a subdomain"""
    try:
        return requests.get("http://" + url_for_test)
    except requests.exceptions.ConnectionError:
        pass


def write_data_to_file(subdomains: list, write_file: str):
    """Write subdomains into file"""
    with open(write_file, 'w') as file_to_write:
        for count_line, line in enumerate(subdomains, 1):
            row = f'{count_line}. {line}\n'
            file_to_write.write(row)


def crawler(target_url: str, subdomains_list: list[str]):
    """Checks the target_url with subdomains from the list. """
    subdomains = []

    for line in subdomains_list:
        test_url = f"{line}.{target_url}"
        response = request(test_url)
        if response:
            print(f"[+] Discovered subdomain ----> {test_url}")
            subdomains.append(test_url)

    return subdomains


def main(target_url: str, wordlist: str, write_file: str = 'subdomains_existing.txt'):
    """Main controller"""
    list_of_subdomains = read_data_from_file(wordlist)

    existing_subdomains = crawler(target_url, list_of_subdomains)
    # print(existing_subdomains)
    # write_data_to_file(existing_subdomains, write_file)


if __name__ == '__main__':
    url = "google.com"
    wordlist_file = "subdomain_dictionaries/subdomain.list"

    main(url, wordlist_file)
