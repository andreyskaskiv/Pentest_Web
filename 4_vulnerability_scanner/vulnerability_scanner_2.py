import re
import urllib.parse

import requests

from handlers.decode_handler import decode_values


class Scanner:
    def __init__(self, url: str, ignore_links: list[str]):
        """
        :param url: target url
        :param ignore_links: a list with ignored links, for example, so as not to lose the session
        """
        self.session = requests.Session()
        self.target_url = url
        self.target_links = []
        self.ignore_links = ignore_links

    def extract_links_from(self, url: str):
        """Extract links from the target site using regular expressions"""
        response = self.session.get(url)
        answer = decode_values(response.content)
        return re.findall('(?:href=")(.*?)"', answer)

    def crawl(self, url=None):
        """Link Refactor. Removes duplicates. Removes links not related to the target site."""
        if url is None:
            url = self.target_url

        href_links = self.extract_links_from(url)
        for link in href_links:
            link = urllib.parse.urljoin(url, link)

            if "#" in link:
                link = link.split("#")[0]

            if (self.target_url in link
                    and link not in self.target_links
                    and link not in self.ignore_links):
                self.target_links.append(link)
                print(link)

                self.crawl(link)


if __name__ == '__main__':
    # target_url = "http://10.0.2.12/mutillidae/"
    target_url = "http://10.0.2.12/dvwa/"
    login = f"{target_url}/login.php"

    links_to_ignore = [
        'http://10.0.2.12/dvwa/logout.php'
    ]

    data_dict = {
        "username": 'admin',
        "password": 'password',
        "Login": 'submit',
    }

    vulnerability_scanner = Scanner(target_url, links_to_ignore)
    # Authorization and session creation
    vulnerability_scanner.session.post(login, data=data_dict)

    vulnerability_scanner.crawl()
